# Описание проекта

## Введение

Проект является количественным исследованием в области анализа временных рядов. Конечная цель исследования - разработка алгоритма, предсказывающего движение рыночных цен.

Подход к исследованию строится на предположении, что рынок имеет предсказуемые периоды, а вся необходимая информация уже содержится в котировках. Очевидно, что рынок иногда бывает детерминирован: выраженные реакции на новости, движения без коррекций дольше обычного, да и в целом случаи резкой "смены поведения", которые никак не объясняются случайным блужданием. У таких аномалий могут иметься предпосылки, являющиеся, по сути, составной частью аномалии, её начальной фазой.

Исследование призвано подтвердить или опровергнуть гипотезу о существовании периодов предсказуемого рынка. И - в случае подтверждения - научиться делать прогноз в такие периоды. Для рынка характерно низкое соотношение "сигнал-шум", а условно-предсказуемые периоды (со слабым сигналом) чередуются с непредсказуемыми периодами (без сигнала). Первые - реже, вторые - чаще. Основная сложность - выявить, извлечь и использовать такой сигнал.

## Стандарты разработки

Автор разрабатывает проект с ноября 2022 года. За это время кодовая база значительно разрослась, а архитектура - усложнилась. Чтобы не превращать код в лоскутное одеяло, которое всё труднее развивать и поддерживать, автор придерживается в разработке следующих правил:

1. Отсутствует или почти отсутствует технический долг: новый функционал бесшовно интегрируется в проект, а сама интеграция прорабатывается сразу от начала до конца, ничего не оставляется "на потом". Иногда это выливается в масштабный рефакторинг.

2. Код насыщен проверками. Например, необходимо гарантировать отсутствие пропусков в массиве, равенство индексов у двух датафреймов, определённую размерность массива и т. п. Всё это в явном виде проверяется. Если проверка не проходит, вызывается исключение, сопровождаемое некоторой отладочной информацией. Иначе подобные ошибки, не выявленные сразу, могут долгое время оставаться незамеченными, так как код в некоторых случаях будет выполняться. Это обессмысливает разработку: например, какой толк от модели, если при её обучении признаки и целевые признаки не соответствовали друг другу? Поэтому проверкам переменных уделяется много внимания. Кроме того, это позволяет ускорить отладку и поиск багов в будущем, так как не придётся повторно проверять то, что уже и так проверено и работает.

3. Аргументы функций (методов), как правило, - позиционные, а именованные - используются лишь в редких обоснованных случаях. Это необходимо для того, чтобы у аргументов не было значений по умолчанию, и при каждом вызове функции (метода) передаваемые в неё значения явно указывались в коде. Улучшается читаемость, снижается вероятность багов.

4. Объектно-ориентированный стиль: активно применяются наследование, инкапсуляция, полиморфизм. При написании кода уделяется внимание его универсальности и возможности дальнейшего переиспользования. Отсутствует или почти отсутствует повторный код. Даже одна-две строки оборачиваются в функцию, если используются многократно.

5. Соблюдение иерархии уровней абстракции. Код иногда выделяется в отдельные функции только лишь с этой целью, даже если используется однократно. То же самое касается классов: без базового класса иногда можно обойтись, но он всё равно определяется, если есть некая группа похожих друг на друга сущностей. Например, имеются метрики и функции потерь. В первом случае в базовом классе реализуются общие для всех метрик методы, поэтому он необходим. Во втором случае базовый класс лишь наследует `_Loss` и больше ничего не делает. Тем не менее, он необходим для иерархии и ещё для одной цели, о которой пойдёт речь ниже. Таким образом, достигается что-то вроде сквозной иерархии: горизонтальное взаимодействие между модулями осуществляется, по возможности, на одном уровне.

6. Функции (методы) со сложным, нетривиальным или не очевидным из названия функционалом всегда сопровождаются подробным описанием, иногда - с примерами. Нет необходимости заново погружаться в старый код: с помощью описания алгоритм функции быстро вспоминается. То же самое касается комментариев в самом коде: если код не самоочевиден, добавляются комментарии.

7. Соблюдается форматирование кода. Стиль приблизительно соответствует форматтеру black.

8. Повсеместно используются подсказки типов данных. Не только при аргументах функции, но и прямо в коде. Например, список строк объявляется так: `a: Iterable[str] = []`. Написание кода от этого не замедляется, а, наоборот, ощутимо ускоряется: IDE всегда знает тип данных и предоставляет соответствующий выпадающий список. Можно узнать, к какому типу относится переменная, просто наведя на неё курсор, а не отслеживая по коду. Минимизируется время на отладку и поддержку. Базовые классы, о которых шла речь выше, хорошо подходят для указания их в качестве типа данных, - это и есть ещё одна их важная роль.

9. Поддерживается принцип нулевой терпимости к костылям в коде, поскольку они вызывают ошибки и приводят к накоплению технического долга. Никаких "захардкодить" в готовом коде оставаться не должно.

## Пайплайн

Исследование состоит из нескольких последовательных этапов, каждый из которых решает определённую задачу: извлечение признаков, отбор признаков, подбор целевого признака, обучение модели, инференс модели (получение данных с её помощью) и т. д. Собственно исследовательская составляющая состоит в том, что правильное решение заранее не известно. Его только предстоит найти - на старте имеется лишь сформулированная задача. Каждый этап - это опробация нескольких различных подходов и сотни ML-экспериментов.

### Скрипт для запуска

Для исключения ошибок (и просто для удобства) разработан shell-скрипт, осуществляющий запуски. Имена блоков, которые надо запустить, передаются через аргумент. Во-первых, так гарантируется запуск блоков в правильной последовательности. Во-вторых, гарантируется правильность аргументов, передаваемых python-скриптам: однажды продуманные и отлаженные, они уже не перепутаются случайно. В-третьих, такой shell-пайплайн годится в качестве исчёрпывающего верхнеуровневого описания проекта в целом, своего рода оглавление.

### Извлечение признаков

Исходные данные - дата-время, OHLC и объём. Признаки - всевозможные статистики и модификации. Некоторые идеи - базовые и очевидные (например, diff и rolling mean), некоторые - почерпнуты из открытых TSA-репозиториев (например, tsfresh), есть также признаки собственной разработки. Дата и время преобразованы через циклические (тригонометрические) функции, поскольку имеют циклический характер. Уделяется внимание недопущению утечек из будущего.

Имеется базовый класс `Calculator` и классы признаков: один признак - один класс.

Для первичного извлечения признаков произвольно выбирается небольшой срез данных (указывается в конфигурации). В ходе первичного извлечения многие признаки сразу отсеиваются, так как не проходят по ограничениям (слишком высокая корреляция, слишком малая дисперсия и т. п.). Те, что остались, проходят отбор с помощью RFE или RFECV (оба алгоритма интегрированы в проект). Информация об извлечении и отборе записывается в файл (таблицу) и затем используется для извлечения признаков из всех остальных данных. По таблице формируется пайплайн, обеспечивающий детерминированое и оптимизированое извлечение: с наименьшим количеством вспомогательных вычислений извлекается только то, что получилось при первичном извлечении и отборе. Под вспомогательными вычислениями здесь понимается вычисление промежуточных признаков, которые сами при отборе были отброшены, но из них рекурсивно извлекаются востребованные признаки.

Работу алгоритмов отбора удалось улучшить за счёт разделения процесса отбора на итерации: признаки отбрасываются малыми порциями. Такой отбор, в свою очередь, повторяется несколько раз, а результаты - определённым образом объединяются.

### Обучение моделей

Для обучения моделей применяется фреймворк собственной разработки. Он прост и функционален: по каждому эксперименту сохраняются метаданные, такие, как лог обучения, конфигурация, архитектура модели, средние и стандартные отклонения признаков (если применяется нормализация). Также сохраняются чекпоинты. Модели сохраняются в готовом к инференсу виде: torchscript и onnx.

## Использование открытых источников

Чтобы не изобретать колесо и не пропускать интересные идеи, автор читает Medium, а когда попадается что-то ценное и заслуживающее изучения, - обращается к первоисточнику на Arxiv.

Так была найдена эффективная архитектура энкодера. К статье на Arxiv прилагается исходный код (репозиторий на Github), поэтому полноценная имплементация в этот раз не потребовалась: достаточно было интегрировать готовый код в проект.

## Эффективность вычислений

Доступное железо: 24 ядра, 64 Гб ОЗУ, RTX2060s 8 Гб.

Эффективно использовать ядра - просто: изначально данные нативно разделены на сравнительно мелкие части, и остаётся только обработать их в нескольких параллельных процессах через `Pool`.

C памятью - сложнее. Данные иногда разрастаются до огромной ширины. Это становится проблемой, когда необходимо кэшировать весь объём данных, но сделать это невозможно из-за нехватки памяти. Проблема решается алгоритмически ценой большего вычислительного времени.

Например, чтобы посчитать стандартное отклонение целого массива, необходимо разделённые на части данные проитерировать дважды: первый раз - для вычисления средних, второй раз - для вычисления дисперсий. При вычислении дисперсии для каждой части массива используется среднее всего массива (предварительно посчитанное в первый проход). Делается так, потому что в конечном итоге необходимо вычислить стандартное отклонение для всего массива целиком, а не, скажем, среднее стандартных отклонений, посчитанных для отдельных частей массива. В этом примере объём вычислений вследствие итерационного характера обработки не изменился. Удвоилось только чтение с диска.

В другом примере - удваивается объём вычислений. Здесь цель - сберечь диск: имеется эксперимент, в ходе которого нужно записать на диск несколько десятков гигабайт промежуточных данных, а провести этот эксперимент требуется многократно. Вычисляется, как и в предыдущем примере, среднее и стандартное отклонение. Но отличие в том, что массив, для которого вычисляются эти статистики, получается в результате инференса модели. Поэтому дважды происходит не просто чтение с диска, а чтение с диска исходных данных плюс обработка их моделью. Чтобы сэкономить машинное время и не делать инференс дважды, можно было бы промежуточные данные записать на диск и затем прочитать их второй раз. Но временные затраты в абсолютном выражении оказались таковы, что ресурс диска - в приоритете: пара десятков экспериментов длительностью 20 минут против 8 минут не стоят убитого диска.

Третий пример - вычисление скользящей корреляции. Имеется массив размером приблизительно 7000 х 10000. Необходимо вычислить скользящую корреляцию по всем возможным комбинациям столбцов по окну размером 100 с шагом 1, что эквивалентно вычислению обычной (не скользящей) корреляции для 6900 массивов размером 100 х 10000. Таким образом, корреляционная матрица будет иметь размер 10000 х 10000 х 6900, - около 2,5 ТБ для float32. Корреляционная матрица нужна целиком, так как задача связана с отбором признаков (нужно удалить коррелирующие). То есть нужно после каждой итерации удаления оценивать оставшиеся признаки во всей их совокупности. Решить задачу помогло использование разреженных матриц. Так как интерес представляет не само значение корреляции, а только факт превышения порога, значения корреляции не кешируются, а только составляется бинарная матрица. Которая, в свою очередь, преобразуется в разреженную и хранится в памяти компактно. Разреженная матрица хранится и обрабатывается по частям. Размер частей подбирается таким, чтобы каждая из частей по отдельности, будучи преобразованной из разреженной в обычную матрицу, помещалась бы в память.

Стоит отметить, что в задаче о скользящей корреляции также пришлось решать проблему ускорения вычислений в целом. Итоговое решение работает примерно на четыре порядка быстрее по сравнению с базовым `pandas.DataFrame.rolling(win).corr()`. Ускорение на два порядка достигается за счёт кэширования знаменателя: вектор, умноженный сам на себя, достаточно вычислить один раз, а не повторять это вычисление N раз для каждого столбца. Ещё два порядка - за счёт переноса вычисления на GPU (код переписан на torch). В итоге то, что оценивалось утилитой tqdm в дни, стало вычисляться за минуты.

## Фишки

1. Конфигурация - это полноценный класс, который сам умеет проверять значения на корректность (например, левая граница интервала не должна быть больше правой); сохраняет конфигурацию на диске (pickle, json, txt); проверяет, чтобы параметры, которые необходимо переопределить, были действительно переопределены. Все эти функции помогают бороться с багами и в целом экономят время: исключение вызывается, как правило, в самом начале исполнения, и сразу становится понятно, где искать проблему, - сообщение об ошибке прямо на неё указывает.

2. `FeatureFrame` - это класс, наследованный от `pandas.DataFrame`, в котором реализованы некоторые дополнительные методы. Стоит честно отметить, что обычный класс-обёртка справился бы не хуже, поэтому решение - спорное. Тем не менее, получилось очень удобно, в коде это выглядит аккуратнее, и просто было интересно поэкспериментировать.

## Дальнейшее развитие проекта

На момент написания данного текста, в декабре 2023 года, исследование завершено на 70-80%. Осталось разработать одну модель, а затем - переходить к тестированию.

### Тестирование

Тестирование состоит из двух частей:

1. Вычисление. Необходимо разработать пайплайн, который принимает на вход исходные данные (дата-время, OHLC и объём) и возвращает прогноз. Все процессы - извлечение признаков, инференс моделей и др. - выполняются внутри. Внимание здесь уделяется прозрачности движения данных: необходимо гарантировать отсутствие утечек (даже самых неявных) из будущего.

2. Анализ. Необходимо исследовать полученный результат с помощью различных метрик. По результатам - сделать вывод, существуют ли периоды предсказуемого рынка.

### Разработка торговой стратегии

Если периоды предсказуемого рынка существуют, то следует разработать торговую стратегию.

Обычно под разработкой торговой стратегии понимается попытка выявить некие закономерности рынка (интуитивно или с применением простейшего математического аппарата), научиться распознавать их и делать по ним прогноз. То есть вручную делать ту работу, которую в данном проекте выполняют модели.

В данном же проекте разработка торговой стратегии - это работа с ответами моделей. Прогноз вычисляется автоматически, а задача сводится к оценке, насколько можно доверять этому прогнозу.
